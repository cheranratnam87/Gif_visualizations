{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "677411dc-cdfd-4c55-9e30-005c8c201c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\3862284329.py:114: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(frame_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GIF saved as: model_comparison_california.gif\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import imageio\n",
    "import glob\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data[:, [0]]  # Feature: MedInc (for simplicity)\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Frame settings\n",
    "n_frames = 20\n",
    "output_dir = \"model_frames_california\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Prediction storage\n",
    "linear_preds, tree_preds, forest_preds, xgb_preds, knn_preds = [], [], [], [], []\n",
    "linear_scores, tree_scores, forest_scores, xgb_scores, knn_scores = [], [], [], [], []\n",
    "\n",
    "# Linear Regression - increasing data\n",
    "for i in range(50, len(X_train), int(len(X_train) / n_frames)):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train[:i], y_train[:i])\n",
    "    preds = model.predict(X_test)\n",
    "    linear_preds.append(preds)\n",
    "    linear_scores.append(r2_score(y_test, preds))\n",
    "\n",
    "# Decision Tree - increasing depth\n",
    "for depth in range(1, n_frames + 1):\n",
    "    model = DecisionTreeRegressor(max_depth=depth)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    tree_preds.append(preds)\n",
    "    tree_scores.append(r2_score(y_test, preds))\n",
    "\n",
    "# Random Forest - increasing n_estimators\n",
    "for n in range(1, n_frames + 1):\n",
    "    model = RandomForestRegressor(n_estimators=n)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    forest_preds.append(preds)\n",
    "    forest_scores.append(r2_score(y_test, preds))\n",
    "\n",
    "# XGBoost - boosting iterations\n",
    "y_pred_total = np.zeros_like(y_test)\n",
    "for i in range(n_frames):\n",
    "    residuals = y_test - y_pred_total\n",
    "    model = XGBRegressor(n_estimators=1, learning_rate=1.0, max_depth=3, verbosity=0)\n",
    "    model.fit(X_test, residuals)\n",
    "    y_pred_total += 0.3 * model.predict(X_test)\n",
    "    xgb_preds.append(y_pred_total.copy())\n",
    "    xgb_scores.append(r2_score(y_test, y_pred_total))\n",
    "\n",
    "# KNN - increasing neighbors\n",
    "for k in range(1, n_frames + 1):\n",
    "    model = KNeighborsRegressor(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    knn_preds.append(preds)\n",
    "    knn_scores.append(r2_score(y_test, preds))\n",
    "\n",
    "# Plot frames\n",
    "for i in range(n_frames):\n",
    "    plt.figure(figsize=(20, 12))\n",
    "\n",
    "    def subplot_model(index, name, y_pred, score, color, annotation):\n",
    "        plt.subplot(3, 2, index)\n",
    "        plt.scatter(X_test, y_test, color='lightgray', alpha=0.4, label='Test Data')\n",
    "        plt.scatter(X_test, y_pred[i], color=color, alpha=0.6, label='Prediction')\n",
    "        plt.title(name, fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(\"Median Income\")\n",
    "        plt.ylabel(\"Target\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.text(0.05, 0.9, f\"R¬≤ Score: {score[i]:.3f}\", transform=plt.gca().transAxes,\n",
    "                 fontsize=10, bbox=dict(facecolor='white', edgecolor='black'))\n",
    "        plt.text(0.05, 0.8, annotation, transform=plt.gca().transAxes,\n",
    "                 fontsize=9, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "    subplot_model(1, \"Linear Regression\", linear_preds, linear_scores, 'red',\n",
    "                  \"Fast, linear, struggles with non-linearity\")\n",
    "    subplot_model(2, \"Decision Tree\", tree_preds, tree_scores, 'blue',\n",
    "                  \"Non-linear splits, interpretable, prone to overfitting\")\n",
    "    subplot_model(3, \"Random Forest\", forest_preds, forest_scores, 'yellow',\n",
    "                  \"Ensemble trees, robust, handles noise\")\n",
    "    subplot_model(4, \"XGBoost\", xgb_preds, xgb_scores, 'blue',\n",
    "                  \"Boosted learners, accurate, optimized\")\n",
    "    subplot_model(5, \"KNN\", knn_preds, knn_scores, 'red',\n",
    "                  \"Local method, sensitive to k\")\n",
    "\n",
    "    plt.suptitle(f\"Model Evolution ‚Äì Frame {i+1}/{n_frames}\", fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(f\"{output_dir}/frame_{i+1:02d}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Create GIF\n",
    "frame_paths = sorted(glob.glob(f\"{output_dir}/frame_*.png\"))\n",
    "gif_path = \"model_comparison_california.gif\"\n",
    "\n",
    "with imageio.get_writer(gif_path, mode='I', duration=0.6, loop=0) as writer:\n",
    "    for frame_path in frame_paths:\n",
    "        image = imageio.imread(frame_path)\n",
    "        writer.append_data(image)\n",
    "\n",
    "print(f\"‚úÖ GIF saved as: {gif_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4eefe77-9d8e-4e10-b1a1-0fdc2b445695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\296855650.py:114: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(frame_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GIF saved as: model_comparison_california.gif\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import imageio\n",
    "import glob\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data[:, [0]]  # Feature: MedInc (for simplicity)\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Frame settings\n",
    "n_frames = 20\n",
    "output_dir = \"model_frames_california\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Prediction storage\n",
    "linear_preds, tree_preds, forest_preds, xgb_preds, knn_preds = [], [], [], [], []\n",
    "linear_scores, tree_scores, forest_scores, xgb_scores, knn_scores = [], [], [], [], []\n",
    "\n",
    "# Linear Regression - increasing data\n",
    "for i in range(50, len(X_train), int(len(X_train) / n_frames)):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train[:i], y_train[:i])\n",
    "    preds = model.predict(X_test)\n",
    "    linear_preds.append(preds)\n",
    "    linear_scores.append(r2_score(y_test, preds))\n",
    "\n",
    "# Decision Tree - increasing depth\n",
    "for depth in range(1, n_frames + 1):\n",
    "    model = DecisionTreeRegressor(max_depth=depth)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    tree_preds.append(preds)\n",
    "    tree_scores.append(r2_score(y_test, preds))\n",
    "\n",
    "# Random Forest - increasing n_estimators\n",
    "for n in range(1, n_frames + 1):\n",
    "    model = RandomForestRegressor(n_estimators=n)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    forest_preds.append(preds)\n",
    "    forest_scores.append(r2_score(y_test, preds))\n",
    "\n",
    "# XGBoost - boosting iterations\n",
    "y_pred_total = np.zeros_like(y_test)\n",
    "for i in range(n_frames):\n",
    "    residuals = y_test - y_pred_total\n",
    "    model = XGBRegressor(n_estimators=1, learning_rate=1.0, max_depth=3, verbosity=0)\n",
    "    model.fit(X_test, residuals)\n",
    "    y_pred_total += 0.3 * model.predict(X_test)\n",
    "    xgb_preds.append(y_pred_total.copy())\n",
    "    xgb_scores.append(r2_score(y_test, y_pred_total))\n",
    "\n",
    "# KNN - increasing neighbors\n",
    "for k in range(1, n_frames + 1):\n",
    "    model = KNeighborsRegressor(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    knn_preds.append(preds)\n",
    "    knn_scores.append(r2_score(y_test, preds))\n",
    "\n",
    "# Plot frames\n",
    "for i in range(n_frames):\n",
    "    plt.figure(figsize=(20, 12))\n",
    "\n",
    "    def subplot_model(index, name, y_pred, score, color, annotation):\n",
    "        plt.subplot(3, 2, index)\n",
    "        plt.scatter(X_test, y_test, color='lightgray', alpha=0.4, label='Test Data')\n",
    "        plt.scatter(X_test, y_pred[i], color=color, alpha=0.6, label='Prediction')\n",
    "        plt.title(name, fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(\"Median Income\")\n",
    "        plt.ylabel(\"Target\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.text(0.05, 0.9, f\"R¬≤ Score: {score[i]:.3f}\", transform=plt.gca().transAxes,\n",
    "                 fontsize=10, bbox=dict(facecolor='white', edgecolor='black'))\n",
    "        plt.text(0.05, 0.8, annotation, transform=plt.gca().transAxes,\n",
    "                 fontsize=9, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "    subplot_model(1, \"Linear Regression\", linear_preds, linear_scores, 'red',\n",
    "                  \"Fast, linear, struggles with non-linearity\")\n",
    "    subplot_model(2, \"Decision Tree\", tree_preds, tree_scores, 'blue',\n",
    "                  \"Non-linear splits, interpretable, prone to overfitting\")\n",
    "    subplot_model(3, \"Random Forest\", forest_preds, forest_scores, 'yellow',\n",
    "                  \"Ensemble trees, robust, handles noise\")\n",
    "    subplot_model(4, \"XGBoost\", xgb_preds, xgb_scores, 'blue',\n",
    "                  \"Boosted learners, accurate, optimized\")\n",
    "    subplot_model(5, \"KNN\", knn_preds, knn_scores, 'red',\n",
    "                  \"Local method, sensitive to k\")\n",
    "\n",
    "    plt.suptitle(f\"Model Evolution ‚Äì Frame {i+1}/{n_frames}\", fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(f\"{output_dir}/frame_{i+1:02d}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Create GIF\n",
    "frame_paths = sorted(glob.glob(f\"{output_dir}/frame_*.png\"))\n",
    "gif_path = \"model_comparison_california.gif\"\n",
    "\n",
    "with imageio.get_writer(gif_path, mode='I', duration=0.6, loop=0) as writer:\n",
    "    for frame_path in frame_paths:\n",
    "        image = imageio.imread(frame_path)\n",
    "        writer.append_data(image)\n",
    "\n",
    "print(f\"‚úÖ GIF saved as: {gif_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23b9668c-7244-4fa0-9d7f-46d4af2fdd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\2519632685.py:136: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(frame_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GIF saved as: finalmodel_comparison_california.gif\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import os\n",
    "import imageio\n",
    "import glob\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data[:, [0]]  # Feature: MedInc (for simplicity)\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Frame settings\n",
    "n_frames = 20\n",
    "output_dir = \"model_frames_california\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Prediction storage\n",
    "linear_preds, tree_preds, forest_preds, xgb_preds, knn_preds = [], [], [], [], []\n",
    "linear_scores, tree_scores, forest_scores, xgb_scores, knn_scores = [], [], [], [], []\n",
    "best_params = {}\n",
    "\n",
    "# Linear Regression - increasing data\n",
    "for i in range(50, len(X_train), int(len(X_train) / n_frames)):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train[:i], y_train[:i])\n",
    "    preds = model.predict(X_test)\n",
    "    linear_preds.append(preds)\n",
    "    linear_scores.append(r2_score(y_test, preds))\n",
    "best_params['Linear Regression'] = {}\n",
    "\n",
    "# Decision Tree - hyperparameter tuning\n",
    "param_grid = {'max_depth': range(1, 21)}\n",
    "grid_search = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params['Decision Tree'] = grid_search.best_params_\n",
    "\n",
    "for depth in range(1, n_frames + 1):\n",
    "    model = DecisionTreeRegressor(max_depth=depth)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    tree_preds.append(preds)\n",
    "    tree_scores.append(r2_score(y_test, preds))\n",
    "\n",
    "# Random Forest - hyperparameter tuning\n",
    "param_grid = {'n_estimators': range(1, 21)}\n",
    "grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params['Random Forest'] = grid_search.best_params_\n",
    "\n",
    "for n in range(1, n_frames + 1):\n",
    "    model = RandomForestRegressor(n_estimators=n)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    forest_preds.append(preds)\n",
    "    forest_scores.append(r2_score(y_test, preds))\n",
    "\n",
    "# XGBoost - hyperparameter tuning\n",
    "param_grid = {'n_estimators': range(1, 21), 'learning_rate': [0.01, 0.1, 0.3, 0.5, 1.0]}\n",
    "grid_search = GridSearchCV(XGBRegressor(max_depth=3, verbosity=0), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params['XGBoost'] = grid_search.best_params_\n",
    "\n",
    "y_pred_total = np.zeros_like(y_test)\n",
    "for i in range(n_frames):\n",
    "    residuals = y_test - y_pred_total\n",
    "    model = XGBRegressor(n_estimators=1, learning_rate=1.0, max_depth=3, verbosity=0)\n",
    "    model.fit(X_test, residuals)\n",
    "    y_pred_total += 0.3 * model.predict(X_test)\n",
    "    xgb_preds.append(y_pred_total.copy())\n",
    "    xgb_scores.append(r2_score(y_test, y_pred_total))\n",
    "\n",
    "# KNN - hyperparameter tuning\n",
    "param_grid = {'n_neighbors': range(1, 21)}\n",
    "grid_search = GridSearchCV(KNeighborsRegressor(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params['KNN'] = grid_search.best_params_\n",
    "\n",
    "for k in range(1, n_frames + 1):\n",
    "    model = KNeighborsRegressor(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    knn_preds.append(preds)\n",
    "    knn_scores.append(r2_score(y_test, preds))\n",
    "\n",
    "# Plot frames\n",
    "for i in range(n_frames):\n",
    "    plt.figure(figsize=(20, 12))\n",
    "\n",
    "    def subplot_model(index, name, y_pred, score, color, annotation):\n",
    "        plt.subplot(3, 2, index)\n",
    "        plt.scatter(X_test, y_test, color='lightgray', alpha=0.4, label='Test Data')\n",
    "        plt.scatter(X_test, y_pred[i], color=color, alpha=0.6, label='Prediction')\n",
    "        plt.title(name, fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(\"Median Income\")\n",
    "        plt.ylabel(\"Target\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.text(0.05, 0.9, f\"R¬≤ Score: {score[i]:.3f}\", transform=plt.gca().transAxes,\n",
    "                 fontsize=10, bbox=dict(facecolor='white', edgecolor='black'))\n",
    "        plt.text(0.05, 0.8, annotation, transform=plt.gca().transAxes,\n",
    "                 fontsize=9, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "    subplot_model(1, \"Linear Regression\", linear_preds, linear_scores, 'red',\n",
    "                  \"Fast, linear, struggles with non-linearity\")\n",
    "    subplot_model(2, \"Decision Tree\", tree_preds, tree_scores, 'blue',\n",
    "                  f\"Non-linear splits, interpretable, prone to overfitting\\nBest Params: {best_params['Decision Tree']}\")\n",
    "    subplot_model(3, \"Random Forest\", forest_preds, forest_scores, 'yellow',\n",
    "                  f\"Ensemble trees, robust, handles noise\\nBest Params: {best_params['Random Forest']}\")\n",
    "    subplot_model(4, \"XGBoost\", xgb_preds, xgb_scores, 'blue',\n",
    "                  f\"Boosted learners, accurate, optimized\\nBest Params: {best_params['XGBoost']}\")\n",
    "    subplot_model(5, \"KNN\", knn_preds, knn_scores, 'red',\n",
    "                  f\"Local method, sensitive to k\\nBest Params: {best_params['KNN']}\")\n",
    "\n",
    "    plt.suptitle(f\"Model Evolution ‚Äì Frame {i+1}/{n_frames}\", fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(f\"{output_dir}/frame_{i+1:02d}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Create GIF\n",
    "frame_paths = sorted(glob.glob(f\"{output_dir}/frame_*.png\"))\n",
    "gif_path = \"finalmodel_comparison_california.gif\"\n",
    "\n",
    "with imageio.get_writer(gif_path, mode='I', duration=0.6, loop=0) as writer:\n",
    "    for frame_path in frame_paths:\n",
    "        image = imageio.imread(frame_path)\n",
    "        writer.append_data(image)\n",
    "\n",
    "print(f\"‚úÖ GIF saved as: {gif_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f28ea3f-67fa-4a62-ac91-b326b9c4adc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\3626230823.py:136: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(frame_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GIF saved as: model_comparison_ames.gif\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import os\n",
    "import imageio\n",
    "import glob\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_openml(name=\"house_prices\", as_frame=True)\n",
    "X = data.data.select_dtypes(include=[np.number]).dropna(axis=1)  # Use numerical features only\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Frame settings\n",
    "n_frames = 20\n",
    "output_dir = \"model_frames_ames\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Prediction storage\n",
    "linear_preds, tree_preds, forest_preds, xgb_preds, knn_preds = [], [], [], [], []\n",
    "linear_scores, tree_scores, forest_scores, xgb_scores, knn_scores = [], [], [], [], []\n",
    "best_params = {}\n",
    "\n",
    "# Linear Regression - increasing data\n",
    "for i in range(50, len(X_train), int(len(X_train) / n_frames)):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train[:i], y_train[:i])\n",
    "    preds = model.predict(X_test)\n",
    "    linear_preds.append(preds)\n",
    "    linear_scores.append(r2_score(y_test, preds))\n",
    "best_params['Linear Regression'] = {}\n",
    "\n",
    "# Decision Tree - hyperparameter tuning\n",
    "param_grid = {'max_depth': range(1, 21)}\n",
    "grid_search = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params['Decision Tree'] = grid_search.best_params_\n",
    "\n",
    "for depth in range(1, n_frames + 1):\n",
    "    model = DecisionTreeRegressor(max_depth=depth)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    tree_preds.append(preds)\n",
    "    tree_scores.append(r2_score(y_test, preds))\n",
    "\n",
    "# Random Forest - hyperparameter tuning\n",
    "param_grid = {'n_estimators': range(1, 21)}\n",
    "grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params['Random Forest'] = grid_search.best_params_\n",
    "\n",
    "for n in range(1, n_frames + 1):\n",
    "    model = RandomForestRegressor(n_estimators=n)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    forest_preds.append(preds)\n",
    "    forest_scores.append(r2_score(y_test, preds))\n",
    "\n",
    "# XGBoost - hyperparameter tuning\n",
    "param_grid = {'n_estimators': range(1, 21), 'learning_rate': [0.01, 0.1, 0.3, 0.5, 1.0]}\n",
    "grid_search = GridSearchCV(XGBRegressor(max_depth=3, verbosity=0), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params['XGBoost'] = grid_search.best_params_\n",
    "\n",
    "y_pred_total = np.zeros_like(y_test, dtype=np.float64)\n",
    "for i in range(n_frames):\n",
    "    residuals = y_test - y_pred_total\n",
    "    model = XGBRegressor(n_estimators=1, learning_rate=1.0, max_depth=3, verbosity=0)\n",
    "    model.fit(X_test, residuals)\n",
    "    y_pred_total += 0.3 * model.predict(X_test)\n",
    "    xgb_preds.append(y_pred_total.copy())\n",
    "    xgb_scores.append(r2_score(y_test, y_pred_total))\n",
    "\n",
    "# KNN - hyperparameter tuning\n",
    "param_grid = {'n_neighbors': range(1, 21)}\n",
    "grid_search = GridSearchCV(KNeighborsRegressor(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params['KNN'] = grid_search.best_params_\n",
    "\n",
    "for k in range(1, n_frames + 1):\n",
    "    model = KNeighborsRegressor(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    knn_preds.append(preds)\n",
    "    knn_scores.append(r2_score(y_test, preds))\n",
    "\n",
    "# Plot frames\n",
    "for i in range(n_frames):\n",
    "    plt.figure(figsize=(20, 12))\n",
    "\n",
    "    def subplot_model(index, name, y_pred, score, color, annotation):\n",
    "        plt.subplot(3, 2, index)\n",
    "        plt.scatter(range(len(y_test)), y_test, color='lightgray', alpha=0.4, label='Test Data')\n",
    "        plt.scatter(range(len(y_test)), y_pred[i], color=color, alpha=0.6, label='Prediction')\n",
    "        plt.title(name, fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(\"Sample Index\")\n",
    "        plt.ylabel(\"Target\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.text(0.05, 0.9, f\"R¬≤ Score: {score[i]:.3f}\", transform=plt.gca().transAxes,\n",
    "                 fontsize=10, bbox=dict(facecolor='white', edgecolor='black'))\n",
    "        plt.text(0.05, 0.8, annotation, transform=plt.gca().transAxes,\n",
    "                 fontsize=9, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "    subplot_model(1, \"Linear Regression\", linear_preds, linear_scores, 'red',\n",
    "                  \"Fast, linear, struggles with non-linearity\")\n",
    "    subplot_model(2, \"Decision Tree\", tree_preds, tree_scores, 'blue',\n",
    "                  f\"Non-linear splits, interpretable, prone to overfitting\\nBest Params: {best_params['Decision Tree']}\")\n",
    "    subplot_model(3, \"Random Forest\", forest_preds, forest_scores, 'yellow',\n",
    "                  f\"Ensemble trees, robust, handles noise\\nBest Params: {best_params['Random Forest']}\")\n",
    "    subplot_model(4, \"XGBoost\", xgb_preds, xgb_scores, 'blue',\n",
    "                  f\"Boosted learners, accurate, optimized\\nBest Params: {best_params['XGBoost']}\")\n",
    "    subplot_model(5, \"KNN\", knn_preds, knn_scores, 'red',\n",
    "                  f\"Local method, sensitive to k\\nBest Params: {best_params['KNN']}\")\n",
    "\n",
    "    plt.suptitle(f\"Model Evolution ‚Äì Frame {i+1}/{n_frames}\", fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(f\"{output_dir}/frame_{i+1:02d}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Create GIF\n",
    "frame_paths = sorted(glob.glob(f\"{output_dir}/frame_*.png\"))\n",
    "gif_path = \"model_comparison_ames.gif\"\n",
    "\n",
    "with imageio.get_writer(gif_path, mode='I', duration=0.6, loop=0) as writer:\n",
    "    for frame_path in frame_paths:\n",
    "        image = imageio.imread(frame_path)\n",
    "        writer.append_data(image)\n",
    "\n",
    "print(f\"‚úÖ GIF saved as: {gif_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "457d29d8-7310-401f-8ca2-7fa7f215af66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\3626230823.py:136: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(frame_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GIF saved as: model_comparison_ames.gif\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import os\n",
    "import imageio\n",
    "import glob\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_openml(name=\"house_prices\", as_frame=True)\n",
    "X = data.data.select_dtypes(include=[np.number]).dropna(axis=1)  # Use numerical features only\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Frame settings\n",
    "n_frames = 20\n",
    "output_dir = \"model_frames_ames\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Prediction storage\n",
    "linear_preds, tree_preds, forest_preds, xgb_preds, knn_preds = [], [], [], [], []\n",
    "linear_scores, tree_scores, forest_scores, xgb_scores, knn_scores = [], [], [], [], []\n",
    "best_params = {}\n",
    "\n",
    "# Linear Regression - increasing data\n",
    "for i in range(50, len(X_train), int(len(X_train) / n_frames)):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train[:i], y_train[:i])\n",
    "    preds = model.predict(X_test)\n",
    "    linear_preds.append(preds)\n",
    "    linear_scores.append(r2_score(y_test, preds))\n",
    "best_params['Linear Regression'] = {}\n",
    "\n",
    "# Decision Tree - hyperparameter tuning\n",
    "param_grid = {'max_depth': range(1, 21)}\n",
    "grid_search = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params['Decision Tree'] = grid_search.best_params_\n",
    "\n",
    "for depth in range(1, n_frames + 1):\n",
    "    model = DecisionTreeRegressor(max_depth=depth)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    tree_preds.append(preds)\n",
    "    tree_scores.append(r2_score(y_test, preds))\n",
    "\n",
    "# Random Forest - hyperparameter tuning\n",
    "param_grid = {'n_estimators': range(1, 21)}\n",
    "grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params['Random Forest'] = grid_search.best_params_\n",
    "\n",
    "for n in range(1, n_frames + 1):\n",
    "    model = RandomForestRegressor(n_estimators=n)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    forest_preds.append(preds)\n",
    "    forest_scores.append(r2_score(y_test, preds))\n",
    "\n",
    "# XGBoost - hyperparameter tuning\n",
    "param_grid = {'n_estimators': range(1, 21), 'learning_rate': [0.01, 0.1, 0.3, 0.5, 1.0]}\n",
    "grid_search = GridSearchCV(XGBRegressor(max_depth=3, verbosity=0), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params['XGBoost'] = grid_search.best_params_\n",
    "\n",
    "y_pred_total = np.zeros_like(y_test, dtype=np.float64)\n",
    "for i in range(n_frames):\n",
    "    residuals = y_test - y_pred_total\n",
    "    model = XGBRegressor(n_estimators=1, learning_rate=1.0, max_depth=3, verbosity=0)\n",
    "    model.fit(X_test, residuals)\n",
    "    y_pred_total += 0.3 * model.predict(X_test)\n",
    "    xgb_preds.append(y_pred_total.copy())\n",
    "    xgb_scores.append(r2_score(y_test, y_pred_total))\n",
    "\n",
    "# KNN - hyperparameter tuning\n",
    "param_grid = {'n_neighbors': range(1, 21)}\n",
    "grid_search = GridSearchCV(KNeighborsRegressor(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params['KNN'] = grid_search.best_params_\n",
    "\n",
    "for k in range(1, n_frames + 1):\n",
    "    model = KNeighborsRegressor(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    knn_preds.append(preds)\n",
    "    knn_scores.append(r2_score(y_test, preds))\n",
    "\n",
    "# Plot frames\n",
    "for i in range(n_frames):\n",
    "    plt.figure(figsize=(20, 12))\n",
    "\n",
    "    def subplot_model(index, name, y_pred, score, color, annotation):\n",
    "        plt.subplot(3, 2, index)\n",
    "        plt.scatter(range(len(y_test)), y_test, color='lightgray', alpha=0.4, label='Test Data')\n",
    "        plt.scatter(range(len(y_test)), y_pred[i], color=color, alpha=0.6, label='Prediction')\n",
    "        plt.title(name, fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(\"Sample Index\")\n",
    "        plt.ylabel(\"Target\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.text(0.05, 0.9, f\"R¬≤ Score: {score[i]:.3f}\", transform=plt.gca().transAxes,\n",
    "                 fontsize=10, bbox=dict(facecolor='white', edgecolor='black'))\n",
    "        plt.text(0.05, 0.8, annotation, transform=plt.gca().transAxes,\n",
    "                 fontsize=9, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "    subplot_model(1, \"Linear Regression\", linear_preds, linear_scores, 'red',\n",
    "                  \"Fast, linear, struggles with non-linearity\")\n",
    "    subplot_model(2, \"Decision Tree\", tree_preds, tree_scores, 'blue',\n",
    "                  f\"Non-linear splits, interpretable, prone to overfitting\\nBest Params: {best_params['Decision Tree']}\")\n",
    "    subplot_model(3, \"Random Forest\", forest_preds, forest_scores, 'yellow',\n",
    "                  f\"Ensemble trees, robust, handles noise\\nBest Params: {best_params['Random Forest']}\")\n",
    "    subplot_model(4, \"XGBoost\", xgb_preds, xgb_scores, 'blue',\n",
    "                  f\"Boosted learners, accurate, optimized\\nBest Params: {best_params['XGBoost']}\")\n",
    "    subplot_model(5, \"KNN\", knn_preds, knn_scores, 'red',\n",
    "                  f\"Local method, sensitive to k\\nBest Params: {best_params['KNN']}\")\n",
    "\n",
    "    plt.suptitle(f\"Model Evolution ‚Äì Frame {i+1}/{n_frames}\", fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(f\"{output_dir}/frame_{i+1:02d}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Create GIF\n",
    "frame_paths = sorted(glob.glob(f\"{output_dir}/frame_*.png\"))\n",
    "gif_path = \"model_comparison_ames.gif\"\n",
    "\n",
    "with imageio.get_writer(gif_path, mode='I', duration=0.6, loop=0) as writer:\n",
    "    for frame_path in frame_paths:\n",
    "        image = imageio.imread(frame_path)\n",
    "        writer.append_data(image)\n",
    "\n",
    "print(f\"‚úÖ GIF saved as: {gif_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc73782c-b15a-42bd-ba9e-1aed9ba8f61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\3309716308.py:136: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(frame_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GIF saved as: model_comparison_california.gif\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import imageio\n",
    "import glob\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data[:, [0]]  # Feature: MedInc (for simplicity)\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Frame settings\n",
    "n_frames = 20\n",
    "output_dir = \"model_frames_california\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Prediction storage\n",
    "linear_preds_train, tree_preds_train, forest_preds_train, xgb_preds_train, knn_preds_train = [], [], [], [], []\n",
    "linear_preds_test, tree_preds_test, forest_preds_test, xgb_preds_test, knn_preds_test = [], [], [], [], []\n",
    "linear_scores_train, tree_scores_train, forest_scores_train, xgb_scores_train, knn_scores_train = [], [], [], [], []\n",
    "linear_scores_test, tree_scores_test, forest_scores_test, xgb_scores_test, knn_scores_test = [], [], [], [], []\n",
    "\n",
    "# Linear Regression - increasing data\n",
    "for i in range(50, len(X_train), int(len(X_train) / n_frames)):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train[:i], y_train[:i])\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    linear_preds_train.append(preds_train)\n",
    "    linear_preds_test.append(preds_test)\n",
    "    linear_scores_train.append(r2_score(y_train, preds_train))\n",
    "    linear_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# Decision Tree - increasing depth\n",
    "for depth in range(1, n_frames + 1):\n",
    "    model = DecisionTreeRegressor(max_depth=depth)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    tree_preds_train.append(preds_train)\n",
    "    tree_preds_test.append(preds_test)\n",
    "    tree_scores_train.append(r2_score(y_train, preds_train))\n",
    "    tree_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# Random Forest - increasing n_estimators\n",
    "for n in range(1, n_frames + 1):\n",
    "    model = RandomForestRegressor(n_estimators=n)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    forest_preds_train.append(preds_train)\n",
    "    forest_preds_test.append(preds_test)\n",
    "    forest_scores_train.append(r2_score(y_train, preds_train))\n",
    "    forest_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# XGBoost - boosting iterations\n",
    "y_pred_total_train = np.zeros_like(y_train)\n",
    "y_pred_total_test = np.zeros_like(y_test)\n",
    "for i in range(n_frames):\n",
    "    residuals_train = y_train - y_pred_total_train\n",
    "    residuals_test = y_test - y_pred_total_test\n",
    "    model = XGBRegressor(n_estimators=1, learning_rate=1.0, max_depth=3, verbosity=0)\n",
    "    model.fit(X_train, residuals_train)\n",
    "    y_pred_total_train += 0.3 * model.predict(X_train)\n",
    "    y_pred_total_test += 0.3 * model.predict(X_test)\n",
    "    xgb_preds_train.append(y_pred_total_train.copy())\n",
    "    xgb_preds_test.append(y_pred_total_test.copy())\n",
    "    xgb_scores_train.append(r2_score(y_train, y_pred_total_train))\n",
    "    xgb_scores_test.append(r2_score(y_test, y_pred_total_test))\n",
    "\n",
    "# KNN - increasing neighbors\n",
    "for k in range(1, n_frames + 1):\n",
    "    model = KNeighborsRegressor(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    knn_preds_train.append(preds_train)\n",
    "    knn_preds_test.append(preds_test)\n",
    "    knn_scores_train.append(r2_score(y_train, preds_train))\n",
    "    knn_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# Plot frames\n",
    "for i in range(n_frames):\n",
    "    plt.figure(figsize=(20, 12))\n",
    "\n",
    "    def subplot_model(index, name, y_pred_train, y_pred_test, score_train, score_test, color, annotation):\n",
    "        plt.subplot(3, 2, index)\n",
    "        plt.scatter(X_test, y_test, color='lightgray', alpha=0.4, label='Test Data')\n",
    "        plt.scatter(X_test, y_pred_test[i], color=color, alpha=0.6, label='Test Prediction')\n",
    "        plt.scatter(X_train, y_pred_train[i], color='green', alpha=0.6, label='Train Prediction')\n",
    "        plt.title(name, fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(\"Median Income\")\n",
    "        plt.ylabel(\"Target\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.text(0.05, 0.9, f\"Train R¬≤: {score_train[i]:.3f}\", transform=plt.gca().transAxes,\n",
    "                 fontsize=10, bbox=dict(facecolor='white', edgecolor='black'))\n",
    "        plt.text(0.05, 0.8, f\"Test R¬≤: {score_test[i]:.3f}\", transform=plt.gca().transAxes,\n",
    "                 fontsize=10, bbox=dict(facecolor='white', edgecolor='black'))\n",
    "        plt.text(0.05, 0.7, annotation, transform=plt.gca().transAxes,\n",
    "                 fontsize=9, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "    subplot_model(1, \"Linear Regression\", linear_preds_train, linear_preds_test, linear_scores_train, linear_scores_test, 'red',\n",
    "                  \"Fast, linear, struggles with non-linearity\")\n",
    "    subplot_model(2, \"Decision Tree\", tree_preds_train, tree_preds_test, tree_scores_train, tree_scores_test, 'blue',\n",
    "                  \"Non-linear splits, interpretable, prone to overfitting\")\n",
    "    subplot_model(3, \"Random Forest\", forest_preds_train, forest_preds_test, forest_scores_train, forest_scores_test, 'yellow',\n",
    "                  \"Ensemble trees, robust, handles noise\")\n",
    "    subplot_model(4, \"XGBoost\", xgb_preds_train, xgb_preds_test, xgb_scores_train, xgb_scores_test, 'blue',\n",
    "                  \"Boosted learners, accurate, optimized\")\n",
    "    subplot_model(5, \"KNN\", knn_preds_train, knn_preds_test, knn_scores_train, knn_scores_test, 'red',\n",
    "                  \"Local method, sensitive to k\")\n",
    "\n",
    "    plt.suptitle(f\"Model Evolution ‚Äì Frame {i+1}/{n_frames}\", fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(f\"{output_dir}/frame_{i+1:02d}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Create GIF\n",
    "frame_paths = sorted(glob.glob(f\"{output_dir}/frame_*.png\"))\n",
    "gif_path = \"model_comparison_california.gif\"\n",
    "\n",
    "with imageio.get_writer(gif_path, mode='I', duration=0.6, loop=0) as writer:\n",
    "    for frame_path in frame_paths:\n",
    "        image = imageio.imread(frame_path)\n",
    "        writer.append_data(image)\n",
    "\n",
    "print(f\"‚úÖ GIF saved as: {gif_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b5f02ac-85ec-4435-bc35-7038b60b64ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\2689472700.py:144: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(frame_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GIF saved as: model_comparison_california.gif\n",
      "üèÜ Best Model: XGBoost with R¬≤ Score: 0.470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\2689472700.py:160: UserWarning: Glyph 127942 (\\N{TROPHY}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(f\"{output_dir}/frame_{n_frames+1:02d}.png\")\n",
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\2689472700.py:166: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(frame_path)\n",
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\2689472700.py:169: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  final_image = imageio.imread(final_frame_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GIF updated with the best model summary: model_comparison_california.gif\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import imageio\n",
    "import glob\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data[:, [0]]  # Feature: MedInc (for simplicity)\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Frame settings\n",
    "n_frames = 20\n",
    "output_dir = \"model_frames_california\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Prediction storage\n",
    "linear_preds_train, tree_preds_train, forest_preds_train, xgb_preds_train, knn_preds_train = [], [], [], [], []\n",
    "linear_preds_test, tree_preds_test, forest_preds_test, xgb_preds_test, knn_preds_test = [], [], [], [], []\n",
    "linear_scores_train, tree_scores_train, forest_scores_train, xgb_scores_train, knn_scores_train = [], [], [], [], []\n",
    "linear_scores_test, tree_scores_test, forest_scores_test, xgb_scores_test, knn_scores_test = [], [], [], [], []\n",
    "\n",
    "# Linear Regression - increasing data\n",
    "for i in range(50, len(X_train), int(len(X_train) / n_frames)):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train[:i], y_train[:i])\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    linear_preds_train.append(preds_train)\n",
    "    linear_preds_test.append(preds_test)\n",
    "    linear_scores_train.append(r2_score(y_train, preds_train))\n",
    "    linear_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# Decision Tree - increasing depth\n",
    "for depth in range(1, n_frames + 1):\n",
    "    model = DecisionTreeRegressor(max_depth=depth)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    tree_preds_train.append(preds_train)\n",
    "    tree_preds_test.append(preds_test)\n",
    "    tree_scores_train.append(r2_score(y_train, preds_train))\n",
    "    tree_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# Random Forest - increasing n_estimators\n",
    "for n in range(1, n_frames + 1):\n",
    "    model = RandomForestRegressor(n_estimators=n)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    forest_preds_train.append(preds_train)\n",
    "    forest_preds_test.append(preds_test)\n",
    "    forest_scores_train.append(r2_score(y_train, preds_train))\n",
    "    forest_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# XGBoost - boosting iterations\n",
    "y_pred_total_train = np.zeros_like(y_train)\n",
    "y_pred_total_test = np.zeros_like(y_test)\n",
    "for i in range(n_frames):\n",
    "    residuals_train = y_train - y_pred_total_train\n",
    "    residuals_test = y_test - y_pred_total_test\n",
    "    model = XGBRegressor(n_estimators=1, learning_rate=1.0, max_depth=3, verbosity=0)\n",
    "    model.fit(X_train, residuals_train)\n",
    "    y_pred_total_train += 0.3 * model.predict(X_train)\n",
    "    y_pred_total_test += 0.3 * model.predict(X_test)\n",
    "    xgb_preds_train.append(y_pred_total_train.copy())\n",
    "    xgb_preds_test.append(y_pred_total_test.copy())\n",
    "    xgb_scores_train.append(r2_score(y_train, y_pred_total_train))\n",
    "    xgb_scores_test.append(r2_score(y_test, y_pred_total_test))\n",
    "\n",
    "# KNN - increasing neighbors\n",
    "for k in range(1, n_frames + 1):\n",
    "    model = KNeighborsRegressor(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    knn_preds_train.append(preds_train)\n",
    "    knn_preds_test.append(preds_test)\n",
    "    knn_scores_train.append(r2_score(y_train, preds_train))\n",
    "    knn_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# Plot frames\n",
    "for i in range(n_frames):\n",
    "    plt.figure(figsize=(20, 12))\n",
    "\n",
    "    def subplot_model(index, name, y_pred_train, y_pred_test, score_train, score_test, color, annotation):\n",
    "        plt.subplot(3, 2, index)\n",
    "        plt.scatter(X_test, y_test, color='lightgray', alpha=0.4, label='Test Data')\n",
    "        plt.scatter(X_test, y_pred_test[i], color=color, alpha=0.6, label='Test Prediction')\n",
    "        plt.scatter(X_train, y_pred_train[i], color='green', alpha=0.6, label='Train Prediction')\n",
    "        plt.title(name, fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(\"Median Income\")\n",
    "        plt.ylabel(\"Target\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.text(0.05, 0.9, f\"Train R¬≤: {score_train[i]:.3f}\", transform=plt.gca().transAxes,\n",
    "                 fontsize=10, bbox=dict(facecolor='white', edgecolor='black'))\n",
    "        plt.text(0.05, 0.8, f\"Test R¬≤: {score_test[i]:.3f}\", transform=plt.gca().transAxes,\n",
    "                 fontsize=10, bbox=dict(facecolor='white', edgecolor='black'))\n",
    "        plt.text(0.05, 0.7, annotation, transform=plt.gca().transAxes,\n",
    "                 fontsize=9, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "    def check_overfitting(score_train, score_test):\n",
    "        if score_train[i] > score_test[i] + 0.1:\n",
    "            return \"Overfitting\"\n",
    "        elif score_train[i] < score_test[i] - 0.1:\n",
    "            return \"Underfitting\"\n",
    "        else:\n",
    "            return \"Good Fit\"\n",
    "\n",
    "    subplot_model(1, \"Linear Regression\", linear_preds_train, linear_preds_test, linear_scores_train, linear_scores_test, 'red',\n",
    "                  f\"Fast, linear, struggles with non-linearity\\n{check_overfitting(linear_scores_train, linear_scores_test)}\")\n",
    "    subplot_model(2, \"Decision Tree\", tree_preds_train, tree_preds_test, tree_scores_train, tree_scores_test, 'blue',\n",
    "                  f\"Non-linear splits, interpretable, prone to overfitting\\n{check_overfitting(tree_scores_train, tree_scores_test)}\")\n",
    "    subplot_model(3, \"Random Forest\", forest_preds_train, forest_preds_test, forest_scores_train, forest_scores_test, 'yellow',\n",
    "                  f\"Ensemble trees, robust, handles noise\\n{check_overfitting(forest_scores_train, forest_scores_test)}\")\n",
    "    subplot_model(4, \"XGBoost\", xgb_preds_train, xgb_preds_test, xgb_scores_train, xgb_scores_test, 'blue',\n",
    "                  f\"Boosted learners, accurate, optimized\\n{check_overfitting(xgb_scores_train, xgb_scores_test)}\")\n",
    "    subplot_model(5, \"KNN\", knn_preds_train, knn_preds_test, knn_scores_train, knn_scores_test, 'red',\n",
    "                  f\"Local method, sensitive to k\\n{check_overfitting(knn_scores_train, knn_scores_test)}\")\n",
    "\n",
    "    plt.suptitle(f\"Model Evolution ‚Äì Frame {i+1}/{n_frames}\", fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(f\"{output_dir}/frame_{i+1:02d}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Create GIF\n",
    "frame_paths = sorted(glob.glob(f\"{output_dir}/frame_*.png\"))\n",
    "gif_path = \"model_comparison_california.gif\"\n",
    "\n",
    "with imageio.get_writer(gif_path, mode='I', duration=0.6, loop=0) as writer:\n",
    "    for frame_path in frame_paths:\n",
    "        image = imageio.imread(frame_path)\n",
    "        writer.append_data(image)\n",
    "\n",
    "# Determine the best model based on the final test R¬≤ score\n",
    "best_model_index = np.argmax([linear_scores_test[-1], tree_scores_test[-1], forest_scores_test[-1], xgb_scores_test[-1], knn_scores_test[-1]])\n",
    "best_model_name = [\"Linear Regression\", \"Decision Tree\", \"Random Forest\", \"XGBoost\", \"KNN\"][best_model_index]\n",
    "best_model_score = [linear_scores_test[-1], tree_scores_test[-1], forest_scores_test[-1], xgb_scores_test[-1], knn_scores_test[-1]][best_model_index]\n",
    "\n",
    "print(f\"‚úÖ GIF saved as: {gif_path}\")\n",
    "print(f\"üèÜ Best Model: {best_model_name} with R¬≤ Score: {best_model_score:.3f}\")\n",
    "\n",
    "# Add final frame with the best model summary\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.text(0.5, 0.5, f\"üèÜ Best Model: {best_model_name}\\n\\nR¬≤ Score: {best_model_score:.3f}\\n\\nWhy: {best_model_name} performed the best because it had the highest test R¬≤ score, indicating it generalizes well to unseen data.\", \n",
    "         fontsize=18, ha='center', va='center', bbox=dict(facecolor='white', edgecolor='black'))\n",
    "plt.axis('off')\n",
    "plt.savefig(f\"{output_dir}/frame_{n_frames+1:02d}.png\")\n",
    "plt.close()\n",
    "\n",
    "# Add the final frame to the GIF\n",
    "with imageio.get_writer(gif_path, mode='I', duration=0.6, loop=0) as writer:\n",
    "    for frame_path in frame_paths:\n",
    "        image = imageio.imread(frame_path)\n",
    "        writer.append_data(image)\n",
    "    final_frame_path = f\"{output_dir}/frame_{n_frames+1:02d}.png\"\n",
    "    final_image = imageio.imread(final_frame_path)\n",
    "    writer.append_data(final_image)\n",
    "\n",
    "print(f\"‚úÖ GIF updated with the best model summary: {gif_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5da08fad-6a11-4c14-a6a2-1c50a5706f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\2161925048.py:144: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(frame_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GIF saved as: model_comparison_ames.gif\n",
      "üèÜ Best Model: XGBoost with R¬≤ Score: 0.883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\2161925048.py:160: UserWarning: Glyph 127942 (\\N{TROPHY}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(f\"{output_dir}/frame_{n_frames+1:02d}.png\")\n",
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\2161925048.py:166: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(frame_path)\n",
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\2161925048.py:169: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  final_image = imageio.imread(final_frame_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GIF updated with the best model summary: model_comparison_ames.gif\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import os\n",
    "import imageio\n",
    "import glob\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_openml(name=\"house_prices\", as_frame=True)\n",
    "X = data.data.select_dtypes(include=[np.number]).dropna(axis=1)  # Use numerical features only\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Frame settings\n",
    "n_frames = 20\n",
    "output_dir = \"model_frames_ames\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Prediction storage\n",
    "linear_preds_train, tree_preds_train, forest_preds_train, xgb_preds_train, knn_preds_train = [], [], [], [], []\n",
    "linear_preds_test, tree_preds_test, forest_preds_test, xgb_preds_test, knn_preds_test = [], [], [], [], []\n",
    "linear_scores_train, tree_scores_train, forest_scores_train, xgb_scores_train, knn_scores_train = [], [], [], [], []\n",
    "linear_scores_test, tree_scores_test, forest_scores_test, xgb_scores_test, knn_scores_test = [], [], [], [], []\n",
    "\n",
    "# Linear Regression - increasing data\n",
    "for i in range(50, len(X_train), int(len(X_train) / n_frames)):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train[:i], y_train[:i])\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    linear_preds_train.append(preds_train)\n",
    "    linear_preds_test.append(preds_test)\n",
    "    linear_scores_train.append(r2_score(y_train, preds_train))\n",
    "    linear_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# Decision Tree - increasing depth\n",
    "for depth in range(1, n_frames + 1):\n",
    "    model = DecisionTreeRegressor(max_depth=depth)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    tree_preds_train.append(preds_train)\n",
    "    tree_preds_test.append(preds_test)\n",
    "    tree_scores_train.append(r2_score(y_train, preds_train))\n",
    "    tree_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# Random Forest - increasing n_estimators\n",
    "for n in range(1, n_frames + 1):\n",
    "    model = RandomForestRegressor(n_estimators=n)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    forest_preds_train.append(preds_train)\n",
    "    forest_preds_test.append(preds_test)\n",
    "    forest_scores_train.append(r2_score(y_train, preds_train))\n",
    "    forest_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# XGBoost - boosting iterations\n",
    "y_pred_total_train = np.zeros_like(y_train, dtype=np.float64)\n",
    "y_pred_total_test = np.zeros_like(y_test, dtype=np.float64)\n",
    "for i in range(n_frames):\n",
    "    residuals_train = y_train - y_pred_total_train\n",
    "    residuals_test = y_test - y_pred_total_test\n",
    "    model = XGBRegressor(n_estimators=1, learning_rate=1.0, max_depth=3, verbosity=0)\n",
    "    model.fit(X_train, residuals_train)\n",
    "    y_pred_total_train += 0.3 * model.predict(X_train)\n",
    "    y_pred_total_test += 0.3 * model.predict(X_test)\n",
    "    xgb_preds_train.append(y_pred_total_train.copy())\n",
    "    xgb_preds_test.append(y_pred_total_test.copy())\n",
    "    xgb_scores_train.append(r2_score(y_train, y_pred_total_train))\n",
    "    xgb_scores_test.append(r2_score(y_test, y_pred_total_test))\n",
    "\n",
    "# KNN - increasing neighbors\n",
    "for k in range(1, n_frames + 1):\n",
    "    model = KNeighborsRegressor(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    knn_preds_train.append(preds_train)\n",
    "    knn_preds_test.append(preds_test)\n",
    "    knn_scores_train.append(r2_score(y_train, preds_train))\n",
    "    knn_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# Plot frames\n",
    "for i in range(n_frames):\n",
    "    plt.figure(figsize=(20, 12))\n",
    "\n",
    "    def subplot_model(index, name, y_pred_train, y_pred_test, score_train, score_test, color, annotation):\n",
    "        plt.subplot(3, 2, index)\n",
    "        plt.scatter(range(len(y_test)), y_test, color='lightgray', alpha=0.4, label='Test Data')\n",
    "        plt.scatter(range(len(y_test)), y_pred_test[i], color=color, alpha=0.6, label='Test Prediction')\n",
    "        plt.scatter(range(len(y_train)), y_pred_train[i], color='green', alpha=0.6, label='Train Prediction')\n",
    "        plt.title(name, fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(\"Sample Index\")\n",
    "        plt.ylabel(\"Target\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.text(0.05, 0.9, f\"Train R¬≤: {score_train[i]:.3f}\", transform=plt.gca().transAxes,\n",
    "                 fontsize=10, bbox=dict(facecolor='white', edgecolor='black'))\n",
    "        plt.text(0.05, 0.8, f\"Test R¬≤: {score_test[i]:.3f}\", transform=plt.gca().transAxes,\n",
    "                 fontsize=10, bbox=dict(facecolor='white', edgecolor='black'))\n",
    "        plt.text(0.05, 0.7, annotation, transform=plt.gca().transAxes,\n",
    "                 fontsize=9, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "    def check_overfitting(score_train, score_test):\n",
    "        if score_train[i] > score_test[i] + 0.1:\n",
    "            return \"Overfitting\"\n",
    "        elif score_train[i] < score_test[i] - 0.1:\n",
    "            return \"Underfitting\"\n",
    "        else:\n",
    "            return \"Good Fit\"\n",
    "\n",
    "    subplot_model(1, \"Linear Regression\", linear_preds_train, linear_preds_test, linear_scores_train, linear_scores_test, 'red',\n",
    "                  f\"Fast, linear, struggles with non-linearity\\n{check_overfitting(linear_scores_train, linear_scores_test)}\")\n",
    "    subplot_model(2, \"Decision Tree\", tree_preds_train, tree_preds_test, tree_scores_train, tree_scores_test, 'blue',\n",
    "                  f\"Non-linear splits, interpretable, prone to overfitting\\n{check_overfitting(tree_scores_train, tree_scores_test)}\")\n",
    "    subplot_model(3, \"Random Forest\", forest_preds_train, forest_preds_test, forest_scores_train, forest_scores_test, 'yellow',\n",
    "                  f\"Ensemble trees, robust, handles noise\\n{check_overfitting(forest_scores_train, forest_scores_test)}\")\n",
    "    subplot_model(4, \"XGBoost\", xgb_preds_train, xgb_preds_test, xgb_scores_train, xgb_scores_test, 'blue',\n",
    "                  f\"Boosted learners, accurate, optimized\\n{check_overfitting(xgb_scores_train, xgb_scores_test)}\")\n",
    "    subplot_model(5, \"KNN\", knn_preds_train, knn_preds_test, knn_scores_train, knn_scores_test, 'red',\n",
    "                  f\"Local method, sensitive to k\\n{check_overfitting(knn_scores_train, knn_scores_test)}\")\n",
    "\n",
    "    plt.suptitle(f\"Model Evolution ‚Äì Frame {i+1}/{n_frames}\", fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(f\"{output_dir}/frame_{i+1:02d}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Create GIF\n",
    "frame_paths = sorted(glob.glob(f\"{output_dir}/frame_*.png\"))\n",
    "gif_path = \"model_comparison_ames.gif\"\n",
    "\n",
    "with imageio.get_writer(gif_path, mode='I', duration=0.6, loop=0) as writer:\n",
    "    for frame_path in frame_paths:\n",
    "        image = imageio.imread(frame_path)\n",
    "        writer.append_data(image)\n",
    "\n",
    "# Determine the best model based on the final test R¬≤ score\n",
    "best_model_index = np.argmax([linear_scores_test[-1], tree_scores_test[-1], forest_scores_test[-1], xgb_scores_test[-1], knn_scores_test[-1]])\n",
    "best_model_name = [\"Linear Regression\", \"Decision Tree\", \"Random Forest\", \"XGBoost\", \"KNN\"][best_model_index]\n",
    "best_model_score = [linear_scores_test[-1], tree_scores_test[-1], forest_scores_test[-1], xgb_scores_test[-1], knn_scores_test[-1]][best_model_index]\n",
    "\n",
    "print(f\"‚úÖ GIF saved as: {gif_path}\")\n",
    "print(f\"üèÜ Best Model: {best_model_name} with R¬≤ Score: {best_model_score:.3f}\")\n",
    "\n",
    "# Add final frame with the best model summary\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.text(0.5, 0.5, f\"üèÜ Best Model: {best_model_name}\\n\\nR¬≤ Score: {best_model_score:.3f}\\n\\nWhy: {best_model_name} performed the best because it had the highest test R¬≤ score, indicating it generalizes well to unseen data.\", \n",
    "         fontsize=18, ha='center', va='center', bbox=dict(facecolor='white', edgecolor='black'))\n",
    "plt.axis('off')\n",
    "plt.savefig(f\"{output_dir}/frame_{n_frames+1:02d}.png\")\n",
    "plt.close()\n",
    "\n",
    "# Add the final frame to the GIF\n",
    "with imageio.get_writer(gif_path, mode='I', duration=0.6, loop=0) as writer:\n",
    "    for frame_path in frame_paths:\n",
    "        image = imageio.imread(frame_path)\n",
    "        writer.append_data(image)\n",
    "    final_frame_path = f\"{output_dir}/frame_{n_frames+1:02d}.png\"\n",
    "    final_image = imageio.imread(final_frame_path)\n",
    "    writer.append_data(final_image)\n",
    "\n",
    "print(f\"‚úÖ GIF updated with the best model summary: {gif_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bc4f119-7f0c-403e-89c3-fb500027d4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\1991539930.py:144: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(frame_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GIF saved as: model_comparison_california.gif\n",
      "üèÜ Best Model: XGBoost with R¬≤ Score: 0.470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\1991539930.py:160: UserWarning: Glyph 127942 (\\N{TROPHY}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(f\"{output_dir}/frame_{n_frames+1:02d}.png\")\n",
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\1991539930.py:166: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(frame_path)\n",
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\1991539930.py:169: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  final_image = imageio.imread(final_frame_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GIF updated with the best model summary: model_comparison_california.gif\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import imageio\n",
    "import glob\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data[:, [0]]  # Feature: MedInc (for simplicity)\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Frame settings\n",
    "n_frames = 20\n",
    "output_dir = \"model_frames_california\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Prediction storage\n",
    "linear_preds_train, tree_preds_train, forest_preds_train, xgb_preds_train, knn_preds_train = [], [], [], [], []\n",
    "linear_preds_test, tree_preds_test, forest_preds_test, xgb_preds_test, knn_preds_test = [], [], [], [], []\n",
    "linear_scores_train, tree_scores_train, forest_scores_train, xgb_scores_train, knn_scores_train = [], [], [], [], []\n",
    "linear_scores_test, tree_scores_test, forest_scores_test, xgb_scores_test, knn_scores_test = [], [], [], [], []\n",
    "\n",
    "# Linear Regression - increasing data\n",
    "for i in range(50, len(X_train), int(len(X_train) / n_frames)):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train[:i], y_train[:i])\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    linear_preds_train.append(preds_train)\n",
    "    linear_preds_test.append(preds_test)\n",
    "    linear_scores_train.append(r2_score(y_train, preds_train))\n",
    "    linear_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# Decision Tree - increasing depth\n",
    "for depth in range(1, n_frames + 1):\n",
    "    model = DecisionTreeRegressor(max_depth=depth)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    tree_preds_train.append(preds_train)\n",
    "    tree_preds_test.append(preds_test)\n",
    "    tree_scores_train.append(r2_score(y_train, preds_train))\n",
    "    tree_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# Random Forest - increasing n_estimators\n",
    "for n in range(1, n_frames + 1):\n",
    "    model = RandomForestRegressor(n_estimators=n)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    forest_preds_train.append(preds_train)\n",
    "    forest_preds_test.append(preds_test)\n",
    "    forest_scores_train.append(r2_score(y_train, preds_train))\n",
    "    forest_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# XGBoost - boosting iterations\n",
    "y_pred_total_train = np.zeros_like(y_train, dtype=np.float64)\n",
    "y_pred_total_test = np.zeros_like(y_test, dtype=np.float64)\n",
    "for i in range(n_frames):\n",
    "    residuals_train = y_train - y_pred_total_train\n",
    "    residuals_test = y_test - y_pred_total_test\n",
    "    model = XGBRegressor(n_estimators=1, learning_rate=1.0, max_depth=3, verbosity=0)\n",
    "    model.fit(X_train, residuals_train)\n",
    "    y_pred_total_train += 0.3 * model.predict(X_train)\n",
    "    y_pred_total_test += 0.3 * model.predict(X_test)\n",
    "    xgb_preds_train.append(y_pred_total_train.copy())\n",
    "    xgb_preds_test.append(y_pred_total_test.copy())\n",
    "    xgb_scores_train.append(r2_score(y_train, y_pred_total_train))\n",
    "    xgb_scores_test.append(r2_score(y_test, y_pred_total_test))\n",
    "\n",
    "# KNN - increasing neighbors\n",
    "for k in range(1, n_frames + 1):\n",
    "    model = KNeighborsRegressor(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    knn_preds_train.append(preds_train)\n",
    "    knn_preds_test.append(preds_test)\n",
    "    knn_scores_train.append(r2_score(y_train, preds_train))\n",
    "    knn_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# Plot frames\n",
    "for i in range(n_frames):\n",
    "    plt.figure(figsize=(20, 12))\n",
    "\n",
    "    def subplot_model(index, name, y_pred_train, y_pred_test, score_train, score_test, color, annotation):\n",
    "        plt.subplot(3, 2, index)\n",
    "        plt.scatter(X_test, y_test, color='lightgray', alpha=0.4, label='Test Data')\n",
    "        plt.scatter(X_test, y_pred_test[i], color=color, alpha=0.6, label='Test Prediction')\n",
    "        plt.scatter(X_train, y_pred_train[i], color='green', alpha=0.6, label='Train Prediction')\n",
    "        plt.title(name, fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(\"Median Income\")\n",
    "        plt.ylabel(\"Target\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.text(0.05, 0.9, f\"Train R¬≤: {score_train[i]:.3f}\", transform=plt.gca().transAxes,\n",
    "                 fontsize=10, bbox=dict(facecolor='white', edgecolor='black'))\n",
    "        plt.text(0.05, 0.8, f\"Test R¬≤: {score_test[i]:.3f}\", transform=plt.gca().transAxes,\n",
    "                 fontsize=10, bbox=dict(facecolor='white', edgecolor='black'))\n",
    "        plt.text(0.05, 0.7, annotation, transform=plt.gca().transAxes,\n",
    "                 fontsize=9, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "    def check_overfitting(score_train, score_test):\n",
    "        if score_train[i] > score_test[i] + 0.1:\n",
    "            return \"Overfitting\"\n",
    "        elif score_train[i] < score_test[i] - 0.1:\n",
    "            return \"Underfitting\"\n",
    "        else:\n",
    "            return \"Good Fit\"\n",
    "\n",
    "    subplot_model(1, \"Linear Regression\", linear_preds_train, linear_preds_test, linear_scores_train, linear_scores_test, 'red',\n",
    "                  f\"Fast, linear, struggles with non-linearity\\n{check_overfitting(linear_scores_train, linear_scores_test)}\")\n",
    "    subplot_model(2, \"Decision Tree\", tree_preds_train, tree_preds_test, tree_scores_train, tree_scores_test, 'blue',\n",
    "                  f\"Non-linear splits, interpretable, prone to overfitting\\n{check_overfitting(tree_scores_train, tree_scores_test)}\")\n",
    "    subplot_model(3, \"Random Forest\", forest_preds_train, forest_preds_test, forest_scores_train, forest_scores_test, 'yellow',\n",
    "                  f\"Ensemble trees, robust, handles noise\\n{check_overfitting(forest_scores_train, forest_scores_test)}\")\n",
    "    subplot_model(4, \"XGBoost\", xgb_preds_train, xgb_preds_test, xgb_scores_train, xgb_scores_test, 'blue',\n",
    "                  f\"Boosted learners, accurate, optimized\\n{check_overfitting(xgb_scores_train, xgb_scores_test)}\")\n",
    "    subplot_model(5, \"KNN\", knn_preds_train, knn_preds_test, knn_scores_train, knn_scores_test, 'red',\n",
    "                  f\"Local method, sensitive to k\\n{check_overfitting(knn_scores_train, knn_scores_test)}\")\n",
    "\n",
    "    plt.suptitle(f\"Model Evolution ‚Äì Frame {i+1}/{n_frames}\", fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(f\"{output_dir}/frame_{i+1:02d}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Create GIF\n",
    "frame_paths = sorted(glob.glob(f\"{output_dir}/frame_*.png\"))\n",
    "gif_path = \"model_comparison_california.gif\"\n",
    "\n",
    "with imageio.get_writer(gif_path, mode='I', duration=0.6, loop=0) as writer:\n",
    "    for frame_path in frame_paths:\n",
    "        image = imageio.imread(frame_path)\n",
    "        writer.append_data(image)\n",
    "\n",
    "# Determine the best model based on the final test R¬≤ score\n",
    "best_model_index = np.argmax([linear_scores_test[-1], tree_scores_test[-1], forest_scores_test[-1], xgb_scores_test[-1], knn_scores_test[-1]])\n",
    "best_model_name = [\"Linear Regression\", \"Decision Tree\", \"Random Forest\", \"XGBoost\", \"KNN\"][best_model_index]\n",
    "best_model_score = [linear_scores_test[-1], tree_scores_test[-1], forest_scores_test[-1], xgb_scores_test[-1], knn_scores_test[-1]][best_model_index]\n",
    "\n",
    "print(f\"‚úÖ GIF saved as: {gif_path}\")\n",
    "print(f\"üèÜ Best Model: {best_model_name} with R¬≤ Score: {best_model_score:.3f}\")\n",
    "\n",
    "# Add final frame with the best model summary\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.text(0.5, 0.5, f\"üèÜ Best Model: {best_model_name}\\n\\nR¬≤ Score: {best_model_score:.3f}\\n\\nWhy: {best_model_name} performed the best because it had the highest test R¬≤ score, indicating it generalizes well to unseen data.\", \n",
    "         fontsize=18, ha='center', va='center', bbox=dict(facecolor='white', edgecolor='black'))\n",
    "plt.axis('off')\n",
    "plt.savefig(f\"{output_dir}/frame_{n_frames+1:02d}.png\")\n",
    "plt.close()\n",
    "\n",
    "# Add the final frame to the GIF\n",
    "with imageio.get_writer(gif_path, mode='I', duration=0.6, loop=0) as writer:\n",
    "    for frame_path in frame_paths:\n",
    "        image = imageio.imread(frame_path)\n",
    "        writer.append_data(image)\n",
    "    final_frame_path = f\"{output_dir}/frame_{n_frames+1:02d}.png\"\n",
    "    final_image = imageio.imread(final_frame_path)\n",
    "    writer.append_data(final_image)\n",
    "\n",
    "print(f\"‚úÖ GIF updated with the best model summary: {gif_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5d094e3-7277-4ca3-8f48-360464ad38b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\1991539930.py:144: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(frame_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GIF saved as: model_comparison_california.gif\n",
      "üèÜ Best Model: XGBoost with R¬≤ Score: 0.470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\1991539930.py:160: UserWarning: Glyph 127942 (\\N{TROPHY}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(f\"{output_dir}/frame_{n_frames+1:02d}.png\")\n",
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\1991539930.py:166: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(frame_path)\n",
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\1991539930.py:169: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  final_image = imageio.imread(final_frame_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GIF updated with the best model summary: model_comparison_california.gif\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import imageio\n",
    "import glob\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data[:, [0]]  # Feature: MedInc (for simplicity)\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Frame settings\n",
    "n_frames = 20\n",
    "output_dir = \"model_frames_california\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Prediction storage\n",
    "linear_preds_train, tree_preds_train, forest_preds_train, xgb_preds_train, knn_preds_train = [], [], [], [], []\n",
    "linear_preds_test, tree_preds_test, forest_preds_test, xgb_preds_test, knn_preds_test = [], [], [], [], []\n",
    "linear_scores_train, tree_scores_train, forest_scores_train, xgb_scores_train, knn_scores_train = [], [], [], [], []\n",
    "linear_scores_test, tree_scores_test, forest_scores_test, xgb_scores_test, knn_scores_test = [], [], [], [], []\n",
    "\n",
    "# Linear Regression - increasing data\n",
    "for i in range(50, len(X_train), int(len(X_train) / n_frames)):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train[:i], y_train[:i])\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    linear_preds_train.append(preds_train)\n",
    "    linear_preds_test.append(preds_test)\n",
    "    linear_scores_train.append(r2_score(y_train, preds_train))\n",
    "    linear_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# Decision Tree - increasing depth\n",
    "for depth in range(1, n_frames + 1):\n",
    "    model = DecisionTreeRegressor(max_depth=depth)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    tree_preds_train.append(preds_train)\n",
    "    tree_preds_test.append(preds_test)\n",
    "    tree_scores_train.append(r2_score(y_train, preds_train))\n",
    "    tree_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# Random Forest - increasing n_estimators\n",
    "for n in range(1, n_frames + 1):\n",
    "    model = RandomForestRegressor(n_estimators=n)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    forest_preds_train.append(preds_train)\n",
    "    forest_preds_test.append(preds_test)\n",
    "    forest_scores_train.append(r2_score(y_train, preds_train))\n",
    "    forest_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# XGBoost - boosting iterations\n",
    "y_pred_total_train = np.zeros_like(y_train, dtype=np.float64)\n",
    "y_pred_total_test = np.zeros_like(y_test, dtype=np.float64)\n",
    "for i in range(n_frames):\n",
    "    residuals_train = y_train - y_pred_total_train\n",
    "    residuals_test = y_test - y_pred_total_test\n",
    "    model = XGBRegressor(n_estimators=1, learning_rate=1.0, max_depth=3, verbosity=0)\n",
    "    model.fit(X_train, residuals_train)\n",
    "    y_pred_total_train += 0.3 * model.predict(X_train)\n",
    "    y_pred_total_test += 0.3 * model.predict(X_test)\n",
    "    xgb_preds_train.append(y_pred_total_train.copy())\n",
    "    xgb_preds_test.append(y_pred_total_test.copy())\n",
    "    xgb_scores_train.append(r2_score(y_train, y_pred_total_train))\n",
    "    xgb_scores_test.append(r2_score(y_test, y_pred_total_test))\n",
    "\n",
    "# KNN - increasing neighbors\n",
    "for k in range(1, n_frames + 1):\n",
    "    model = KNeighborsRegressor(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    knn_preds_train.append(preds_train)\n",
    "    knn_preds_test.append(preds_test)\n",
    "    knn_scores_train.append(r2_score(y_train, preds_train))\n",
    "    knn_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# Plot frames\n",
    "for i in range(n_frames):\n",
    "    plt.figure(figsize=(20, 12))\n",
    "\n",
    "    def subplot_model(index, name, y_pred_train, y_pred_test, score_train, score_test, color, annotation):\n",
    "        plt.subplot(3, 2, index)\n",
    "        plt.scatter(X_test, y_test, color='lightgray', alpha=0.4, label='Test Data')\n",
    "        plt.scatter(X_test, y_pred_test[i], color=color, alpha=0.6, label='Test Prediction')\n",
    "        plt.scatter(X_train, y_pred_train[i], color='green', alpha=0.6, label='Train Prediction')\n",
    "        plt.title(name, fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(\"Median Income\")\n",
    "        plt.ylabel(\"Target\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.text(0.05, 0.9, f\"Train R¬≤: {score_train[i]:.3f}\", transform=plt.gca().transAxes,\n",
    "                 fontsize=10, bbox=dict(facecolor='white', edgecolor='black'))\n",
    "        plt.text(0.05, 0.8, f\"Test R¬≤: {score_test[i]:.3f}\", transform=plt.gca().transAxes,\n",
    "                 fontsize=10, bbox=dict(facecolor='white', edgecolor='black'))\n",
    "        plt.text(0.05, 0.7, annotation, transform=plt.gca().transAxes,\n",
    "                 fontsize=9, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "    def check_overfitting(score_train, score_test):\n",
    "        if score_train[i] > score_test[i] + 0.1:\n",
    "            return \"Overfitting\"\n",
    "        elif score_train[i] < score_test[i] - 0.1:\n",
    "            return \"Underfitting\"\n",
    "        else:\n",
    "            return \"Good Fit\"\n",
    "\n",
    "    subplot_model(1, \"Linear Regression\", linear_preds_train, linear_preds_test, linear_scores_train, linear_scores_test, 'red',\n",
    "                  f\"Fast, linear, struggles with non-linearity\\n{check_overfitting(linear_scores_train, linear_scores_test)}\")\n",
    "    subplot_model(2, \"Decision Tree\", tree_preds_train, tree_preds_test, tree_scores_train, tree_scores_test, 'blue',\n",
    "                  f\"Non-linear splits, interpretable, prone to overfitting\\n{check_overfitting(tree_scores_train, tree_scores_test)}\")\n",
    "    subplot_model(3, \"Random Forest\", forest_preds_train, forest_preds_test, forest_scores_train, forest_scores_test, 'yellow',\n",
    "                  f\"Ensemble trees, robust, handles noise\\n{check_overfitting(forest_scores_train, forest_scores_test)}\")\n",
    "    subplot_model(4, \"XGBoost\", xgb_preds_train, xgb_preds_test, xgb_scores_train, xgb_scores_test, 'blue',\n",
    "                  f\"Boosted learners, accurate, optimized\\n{check_overfitting(xgb_scores_train, xgb_scores_test)}\")\n",
    "    subplot_model(5, \"KNN\", knn_preds_train, knn_preds_test, knn_scores_train, knn_scores_test, 'red',\n",
    "                  f\"Local method, sensitive to k\\n{check_overfitting(knn_scores_train, knn_scores_test)}\")\n",
    "\n",
    "    plt.suptitle(f\"Model Evolution ‚Äì Frame {i+1}/{n_frames}\", fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(f\"{output_dir}/frame_{i+1:02d}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Create GIF\n",
    "frame_paths = sorted(glob.glob(f\"{output_dir}/frame_*.png\"))\n",
    "gif_path = \"model_comparison_california.gif\"\n",
    "\n",
    "with imageio.get_writer(gif_path, mode='I', duration=0.6, loop=0) as writer:\n",
    "    for frame_path in frame_paths:\n",
    "        image = imageio.imread(frame_path)\n",
    "        writer.append_data(image)\n",
    "\n",
    "# Determine the best model based on the final test R¬≤ score\n",
    "best_model_index = np.argmax([linear_scores_test[-1], tree_scores_test[-1], forest_scores_test[-1], xgb_scores_test[-1], knn_scores_test[-1]])\n",
    "best_model_name = [\"Linear Regression\", \"Decision Tree\", \"Random Forest\", \"XGBoost\", \"KNN\"][best_model_index]\n",
    "best_model_score = [linear_scores_test[-1], tree_scores_test[-1], forest_scores_test[-1], xgb_scores_test[-1], knn_scores_test[-1]][best_model_index]\n",
    "\n",
    "print(f\"‚úÖ GIF saved as: {gif_path}\")\n",
    "print(f\"üèÜ Best Model: {best_model_name} with R¬≤ Score: {best_model_score:.3f}\")\n",
    "\n",
    "# Add final frame with the best model summary\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.text(0.5, 0.5, f\"üèÜ Best Model: {best_model_name}\\n\\nR¬≤ Score: {best_model_score:.3f}\\n\\nWhy: {best_model_name} performed the best because it had the highest test R¬≤ score, indicating it generalizes well to unseen data.\", \n",
    "         fontsize=18, ha='center', va='center', bbox=dict(facecolor='white', edgecolor='black'))\n",
    "plt.axis('off')\n",
    "plt.savefig(f\"{output_dir}/frame_{n_frames+1:02d}.png\")\n",
    "plt.close()\n",
    "\n",
    "# Add the final frame to the GIF\n",
    "with imageio.get_writer(gif_path, mode='I', duration=0.6, loop=0) as writer:\n",
    "    for frame_path in frame_paths:\n",
    "        image = imageio.imread(frame_path)\n",
    "        writer.append_data(image)\n",
    "    final_frame_path = f\"{output_dir}/frame_{n_frames+1:02d}.png\"\n",
    "    final_image = imageio.imread(final_frame_path)\n",
    "    writer.append_data(final_image)\n",
    "\n",
    "print(f\"‚úÖ GIF updated with the best model summary: {gif_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2508a588-027a-4287-9fce-b7d5f0120734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\1991539930.py:144: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(frame_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GIF saved as: model_comparison_california.gif\n",
      "üèÜ Best Model: XGBoost with R¬≤ Score: 0.470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\1991539930.py:160: UserWarning: Glyph 127942 (\\N{TROPHY}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(f\"{output_dir}/frame_{n_frames+1:02d}.png\")\n",
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\1991539930.py:166: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(frame_path)\n",
      "C:\\Users\\chera\\AppData\\Local\\Temp\\ipykernel_13776\\1991539930.py:169: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  final_image = imageio.imread(final_frame_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GIF updated with the best model summary: model_comparison_california.gif\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import imageio\n",
    "import glob\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data[:, [0]]  # Feature: MedInc (for simplicity)\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Frame settings\n",
    "n_frames = 20\n",
    "output_dir = \"model_frames_california\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Prediction storage\n",
    "linear_preds_train, tree_preds_train, forest_preds_train, xgb_preds_train, knn_preds_train = [], [], [], [], []\n",
    "linear_preds_test, tree_preds_test, forest_preds_test, xgb_preds_test, knn_preds_test = [], [], [], [], []\n",
    "linear_scores_train, tree_scores_train, forest_scores_train, xgb_scores_train, knn_scores_train = [], [], [], [], []\n",
    "linear_scores_test, tree_scores_test, forest_scores_test, xgb_scores_test, knn_scores_test = [], [], [], [], []\n",
    "\n",
    "# Linear Regression - increasing data\n",
    "for i in range(50, len(X_train), int(len(X_train) / n_frames)):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train[:i], y_train[:i])\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    linear_preds_train.append(preds_train)\n",
    "    linear_preds_test.append(preds_test)\n",
    "    linear_scores_train.append(r2_score(y_train, preds_train))\n",
    "    linear_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# Decision Tree - increasing depth\n",
    "for depth in range(1, n_frames + 1):\n",
    "    model = DecisionTreeRegressor(max_depth=depth)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    tree_preds_train.append(preds_train)\n",
    "    tree_preds_test.append(preds_test)\n",
    "    tree_scores_train.append(r2_score(y_train, preds_train))\n",
    "    tree_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# Random Forest - increasing n_estimators\n",
    "for n in range(1, n_frames + 1):\n",
    "    model = RandomForestRegressor(n_estimators=n)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    forest_preds_train.append(preds_train)\n",
    "    forest_preds_test.append(preds_test)\n",
    "    forest_scores_train.append(r2_score(y_train, preds_train))\n",
    "    forest_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# XGBoost - boosting iterations\n",
    "y_pred_total_train = np.zeros_like(y_train, dtype=np.float64)\n",
    "y_pred_total_test = np.zeros_like(y_test, dtype=np.float64)\n",
    "for i in range(n_frames):\n",
    "    residuals_train = y_train - y_pred_total_train\n",
    "    residuals_test = y_test - y_pred_total_test\n",
    "    model = XGBRegressor(n_estimators=1, learning_rate=1.0, max_depth=3, verbosity=0)\n",
    "    model.fit(X_train, residuals_train)\n",
    "    y_pred_total_train += 0.3 * model.predict(X_train)\n",
    "    y_pred_total_test += 0.3 * model.predict(X_test)\n",
    "    xgb_preds_train.append(y_pred_total_train.copy())\n",
    "    xgb_preds_test.append(y_pred_total_test.copy())\n",
    "    xgb_scores_train.append(r2_score(y_train, y_pred_total_train))\n",
    "    xgb_scores_test.append(r2_score(y_test, y_pred_total_test))\n",
    "\n",
    "# KNN - increasing neighbors\n",
    "for k in range(1, n_frames + 1):\n",
    "    model = KNeighborsRegressor(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_test = model.predict(X_test)\n",
    "    knn_preds_train.append(preds_train)\n",
    "    knn_preds_test.append(preds_test)\n",
    "    knn_scores_train.append(r2_score(y_train, preds_train))\n",
    "    knn_scores_test.append(r2_score(y_test, preds_test))\n",
    "\n",
    "# Plot frames\n",
    "for i in range(n_frames):\n",
    "    plt.figure(figsize=(20, 12))\n",
    "\n",
    "    def subplot_model(index, name, y_pred_train, y_pred_test, score_train, score_test, color, annotation):\n",
    "        plt.subplot(3, 2, index)\n",
    "        plt.scatter(X_test, y_test, color='lightgray', alpha=0.4, label='Test Data')\n",
    "        plt.scatter(X_test, y_pred_test[i], color=color, alpha=0.6, label='Test Prediction')\n",
    "        plt.scatter(X_train, y_pred_train[i], color='green', alpha=0.6, label='Train Prediction')\n",
    "        plt.title(name, fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(\"Median Income\")\n",
    "        plt.ylabel(\"Target\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.text(0.05, 0.9, f\"Train R¬≤: {score_train[i]:.3f}\", transform=plt.gca().transAxes,\n",
    "                 fontsize=10, bbox=dict(facecolor='white', edgecolor='black'))\n",
    "        plt.text(0.05, 0.8, f\"Test R¬≤: {score_test[i]:.3f}\", transform=plt.gca().transAxes,\n",
    "                 fontsize=10, bbox=dict(facecolor='white', edgecolor='black'))\n",
    "        plt.text(0.05, 0.7, annotation, transform=plt.gca().transAxes,\n",
    "                 fontsize=9, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "    def check_overfitting(score_train, score_test):\n",
    "        if score_train[i] > score_test[i] + 0.1:\n",
    "            return \"Overfitting\"\n",
    "        elif score_train[i] < score_test[i] - 0.1:\n",
    "            return \"Underfitting\"\n",
    "        else:\n",
    "            return \"Good Fit\"\n",
    "\n",
    "    subplot_model(1, \"Linear Regression\", linear_preds_train, linear_preds_test, linear_scores_train, linear_scores_test, 'red',\n",
    "                  f\"Fast, linear, struggles with non-linearity\\n{check_overfitting(linear_scores_train, linear_scores_test)}\")\n",
    "    subplot_model(2, \"Decision Tree\", tree_preds_train, tree_preds_test, tree_scores_train, tree_scores_test, 'blue',\n",
    "                  f\"Non-linear splits, interpretable, prone to overfitting\\n{check_overfitting(tree_scores_train, tree_scores_test)}\")\n",
    "    subplot_model(3, \"Random Forest\", forest_preds_train, forest_preds_test, forest_scores_train, forest_scores_test, 'yellow',\n",
    "                  f\"Ensemble trees, robust, handles noise\\n{check_overfitting(forest_scores_train, forest_scores_test)}\")\n",
    "    subplot_model(4, \"XGBoost\", xgb_preds_train, xgb_preds_test, xgb_scores_train, xgb_scores_test, 'blue',\n",
    "                  f\"Boosted learners, accurate, optimized\\n{check_overfitting(xgb_scores_train, xgb_scores_test)}\")\n",
    "    subplot_model(5, \"KNN\", knn_preds_train, knn_preds_test, knn_scores_train, knn_scores_test, 'red',\n",
    "                  f\"Local method, sensitive to k\\n{check_overfitting(knn_scores_train, knn_scores_test)}\")\n",
    "\n",
    "    plt.suptitle(f\"Model Evolution ‚Äì Frame {i+1}/{n_frames}\", fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(f\"{output_dir}/frame_{i+1:02d}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Create GIF\n",
    "frame_paths = sorted(glob.glob(f\"{output_dir}/frame_*.png\"))\n",
    "gif_path = \"model_comparison_california.gif\"\n",
    "\n",
    "with imageio.get_writer(gif_path, mode='I', duration=0.6, loop=0) as writer:\n",
    "    for frame_path in frame_paths:\n",
    "        image = imageio.imread(frame_path)\n",
    "        writer.append_data(image)\n",
    "\n",
    "# Determine the best model based on the final test R¬≤ score\n",
    "best_model_index = np.argmax([linear_scores_test[-1], tree_scores_test[-1], forest_scores_test[-1], xgb_scores_test[-1], knn_scores_test[-1]])\n",
    "best_model_name = [\"Linear Regression\", \"Decision Tree\", \"Random Forest\", \"XGBoost\", \"KNN\"][best_model_index]\n",
    "best_model_score = [linear_scores_test[-1], tree_scores_test[-1], forest_scores_test[-1], xgb_scores_test[-1], knn_scores_test[-1]][best_model_index]\n",
    "\n",
    "print(f\"‚úÖ GIF saved as: {gif_path}\")\n",
    "print(f\"üèÜ Best Model: {best_model_name} with R¬≤ Score: {best_model_score:.3f}\")\n",
    "\n",
    "# Add final frame with the best model summary\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.text(0.5, 0.5, f\"üèÜ Best Model: {best_model_name}\\n\\nR¬≤ Score: {best_model_score:.3f}\\n\\nWhy: {best_model_name} performed the best because it had the highest test R¬≤ score, indicating it generalizes well to unseen data.\", \n",
    "         fontsize=18, ha='center', va='center', bbox=dict(facecolor='white', edgecolor='black'))\n",
    "plt.axis('off')\n",
    "plt.savefig(f\"{output_dir}/frame_{n_frames+1:02d}.png\")\n",
    "plt.close()\n",
    "\n",
    "# Add the final frame to the GIF\n",
    "with imageio.get_writer(gif_path, mode='I', duration=0.6, loop=0) as writer:\n",
    "    for frame_path in frame_paths:\n",
    "        image = imageio.imread(frame_path)\n",
    "        writer.append_data(image)\n",
    "    final_frame_path = f\"{output_dir}/frame_{n_frames+1:02d}.png\"\n",
    "    final_image = imageio.imread(final_frame_path)\n",
    "    writer.append_data(final_image)\n",
    "\n",
    "print(f\"‚úÖ GIF updated with the best model summary: {gif_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea5d7bf-ea60-4e99-abeb-51b0c911e4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
